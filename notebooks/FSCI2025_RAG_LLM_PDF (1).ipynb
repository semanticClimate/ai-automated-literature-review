{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Before using this Colab, please save a copy to your own Google Drive:\n",
        "Click on “File” > “Save a copy in Drive”**\n",
        "\n",
        "# **AI Assisted Literature Review Part II RAG/LLM**\n",
        "# *A. Download Research Papers of interest*\n",
        "# *B. Demo:Pre-process the downloaded file*   \n",
        "# *C. Demo:Query your newly created RAG/LLM*\n",
        "\n",
        "\n",
        "# This Colab notebook processes scientific papers, extracts metadata, creates a searchable vector database, and enables interactive question-answering using the Retrieval-Augmented Generation (RAG) approach with Groq's LLM. You can easily query the system for answers based on the processed documents."
      ],
      "metadata": {
        "id": "5x46V16Bj9Pw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **WORKFLOW:**\n",
        "* Install necessary libraries.\n",
        "* Set Groq API Key\n",
        "* Download research paper using [pygetpapers](https://github.com/petermr/pygetpapers)\n",
        "* Parse XML Files to Markdown and Extract Metadata\n",
        "* Create Vector Database\n",
        "* Execute pipeline for Processing and Retrieval\n",
        "* Query your newly created RAG/LLM"
      ],
      "metadata": {
        "id": "IRdMWdLO5t2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1: Install dependencies**\n",
        "* **pymupdf4llm:** Lightweight PDF processing for LLMs\n",
        "* **langchain:** Framework for developing LLM-powered applications\n",
        "* **chromadb:** Vector store for storing and querying embeddings\n",
        "* **sentence-transformers:** For embedding sentences using transformer models\n",
        "\n"
      ],
      "metadata": {
        "id": "X_xk4yHyceoU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2Wrrkyz25pm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c64eec97-7170-4be7-c637-db060d9a9260",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pygetpapers\n",
            "  Downloading pygetpapers-1.2.5-py3-none-any.whl.metadata (48 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/48.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pygetpapers) (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from pygetpapers) (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from pygetpapers) (5.4.0)\n",
            "Collecting xmltodict (from pygetpapers)\n",
            "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting configargparse (from pygetpapers)\n",
            "  Downloading configargparse-1.7.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting habanero (from pygetpapers)\n",
            "  Downloading habanero-2.3.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting arxiv (from pygetpapers)\n",
            "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting dict2xml (from pygetpapers)\n",
            "  Downloading dict2xml-1.7.7-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pygetpapers) (4.67.1)\n",
            "Collecting coloredlogs (from pygetpapers)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv->pygetpapers)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pygetpapers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pygetpapers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pygetpapers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pygetpapers) (2025.7.14)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->pygetpapers)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: httpx>=0.27.2 in /usr/local/lib/python3.11/dist-packages (from habanero->pygetpapers) (0.28.1)\n",
            "Requirement already satisfied: packaging>=24.1 in /usr/local/lib/python3.11/dist-packages (from habanero->pygetpapers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from habanero->pygetpapers) (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pygetpapers) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pygetpapers) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->pygetpapers) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->pygetpapers) (2025.2)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv->pygetpapers)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->habanero->pygetpapers) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->habanero->pygetpapers) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.2->habanero->pygetpapers) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->pygetpapers) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.2->habanero->pygetpapers) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.2->habanero->pygetpapers) (4.14.1)\n",
            "Downloading pygetpapers-1.2.5-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.4/111.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading configargparse-1.7.1-py3-none-any.whl (25 kB)\n",
            "Downloading dict2xml-1.7.7-py3-none-any.whl (7.9 kB)\n",
            "Downloading habanero-2.3.0-py3-none-any.whl (28 kB)\n",
            "Downloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=63e724c083afb0db406bcb152c804a61887c5c25ccb9706e38aece117771af66\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, xmltodict, humanfriendly, feedparser, dict2xml, configargparse, coloredlogs, arxiv, habanero, pygetpapers\n",
            "Successfully installed arxiv-2.2.0 coloredlogs-15.0.1 configargparse-1.7.1 dict2xml-1.7.7 feedparser-6.0.11 habanero-2.3.0 humanfriendly-10.0 pygetpapers-1.2.5 sgmllib3k-1.0.0 xmltodict-0.14.2\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.69)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.1)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (8.5.0)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.11.0)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.35.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.35.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.35.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.56b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m111.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.35.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.35.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=9ac4b804d55fa5feb8ff186463ae3d617e3c7fd73bfad1ee0565efda05232979\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, python-dotenv, pybase64, overrides, opentelemetry-proto, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mmh3, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, nvidia-cusparse-cu12, nvidia-cudnn-cu12, opentelemetry-semantic-conventions, nvidia-cusolver-cu12, kubernetes, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.15 durationpy-0.10 httptools-0.6.4 kubernetes-33.1.0 mmh3-5.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnxruntime-1.22.1 opentelemetry-api-1.35.0 opentelemetry-exporter-otlp-proto-common-1.35.0 opentelemetry-exporter-otlp-proto-grpc-1.35.0 opentelemetry-proto-1.35.0 opentelemetry-sdk-1.35.0 opentelemetry-semantic-conventions-0.56b0 overrides-7.7.0 posthog-5.4.0 pybase64-1.4.1 pypika-0.48.9 python-dotenv-1.1.1 uvloop-0.21.0 watchfiles-1.1.0\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.3.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.69)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.6)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Collecting groq<1,>=0.29.0 (from langchain-groq)\n",
            "  Downloading groq-0.30.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain-groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain-groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain-groq) (4.14.1)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (25.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.29.0->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.29.0->langchain-groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.29.0->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.29.0->langchain-groq) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-0.3.6-py3-none-any.whl (16 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading groq-0.30.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, groq, dataclasses-json, langchain-groq, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 groq-0.30.0 httpx-sse-0.4.1 langchain-community-0.3.27 langchain-groq-0.3.6 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 typing-inspect-0.9.0\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting markdown2\n",
            "  Downloading markdown2-2.5.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting weasyprint\n",
            "  Downloading weasyprint-65.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting pydyf>=0.11.0 (from weasyprint)\n",
            "  Downloading pydyf-0.11.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.11/dist-packages (from weasyprint) (1.17.1)\n",
            "Collecting tinyhtml5>=2.0.0b1 (from weasyprint)\n",
            "  Downloading tinyhtml5-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: tinycss2>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from weasyprint) (1.4.0)\n",
            "Collecting cssselect2>=0.8.0 (from weasyprint)\n",
            "  Downloading cssselect2-0.8.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting Pyphen>=0.9.1 (from weasyprint)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1.0 in /usr/local/lib/python3.11/dist-packages (from weasyprint) (11.2.1)\n",
            "Requirement already satisfied: fonttools>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from fonttools[woff]>=4.0.0->weasyprint) (4.58.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=0.6->weasyprint) (2.22)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from cssselect2>=0.8.0->weasyprint) (0.5.1)\n",
            "Collecting brotli>=1.0.1 (from fonttools[woff]>=4.0.0->weasyprint)\n",
            "  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting zopfli>=0.1.4 (from fonttools[woff]>=4.0.0->weasyprint)\n",
            "  Downloading zopfli-0.2.3.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
            "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown2-2.5.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading weasyprint-65.1-py3-none-any.whl (298 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect2-0.8.0-py3-none-any.whl (15 kB)\n",
            "Downloading pydyf-0.11.0-py3-none-any.whl (8.1 kB)\n",
            "Downloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tinyhtml5-2.0.0-py3-none-any.whl (39 kB)\n",
            "Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zopfli-0.2.3.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (850 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m850.6/850.6 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: brotli, zopfli, tinyhtml5, Pyphen, pymupdf, pydyf, markdown2, cssselect2, weasyprint\n",
            "Successfully installed Pyphen-0.17.2 brotli-1.1.0 cssselect2-0.8.0 markdown2-2.5.3 pydyf-0.11.0 pymupdf-1.26.3 tinyhtml5-2.0.0 weasyprint-65.1 zopfli-0.2.3.post1\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install pygetpapers\n",
        "!pip install lxml\n",
        "!pip install langchain chromadb sentence-transformers\n",
        "!pip install -U langchain-huggingface\n",
        "!pip install -U langchain-community langchain-groq\n",
        "!pip install pymupdf markdown2 weasyprint"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2: Set Groq API Key**\n",
        "Groq’s LPU (Language Processing Unit) hardware enables real-time, low-latency responses from LLMs—ideal for interactive applications.\n",
        "\n",
        "**Instructions:**\n",
        "* Go to https://console.groq.com/\n",
        "\n",
        "* Create an account if you don’t have one\n",
        "\n",
        "* Generate your API token\n",
        "\n",
        "* Copy and paste it when prompted below\n",
        "* CLICK **ENTER** once done."
      ],
      "metadata": {
        "id": "mThWgh0yeAmw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2To2JYO28K6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c69265bf-22ea-4317-c078-627a23b6773d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔐 Enter your Groq API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "#  Set API Key\n",
        "import os, getpass\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"🔐 Enter your Groq API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3: Download Research Papers**\n",
        "Use *pygetpapers* to fetch research articles related to the keyword."
      ],
      "metadata": {
        "id": "6E-ULkcfe8RF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtqyqV9Y3EdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d32337-4b5f-494e-80db-c0608e647871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30mINFO:\u001b[0m Total Hits are 251138\n",
            "\r0it [00:00, ?it/s]\r2it [00:00, 32140.26it/s]\n",
            "\u001b[1;30mINFO:\u001b[0m Saving XML files to /content/data_climate/*/fulltext.xml\n",
            "100% 2/2 [00:01<00:00,  1.14it/s]\n"
          ]
        }
      ],
      "source": [
        "# Download papers from EuropePMC\n",
        "!pygetpapers --query '\"Climate change\"' --xml --limit 2 --output /content/data_climate --save_query"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 4: Parse XML Files to Markdown and Extract Metadata**\n",
        "Convert scientific articles (downloaded in XML format) into clean Markdown format and extract essential metadata like title, authors, and DOI."
      ],
      "metadata": {
        "id": "RSnSFCpXf_HK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hljg_PRO3IwT"
      },
      "outputs": [],
      "source": [
        "#  Parse XMLs to Markdown and extract metadata\n",
        "import pathlib\n",
        "import re\n",
        "from lxml import etree\n",
        "import fitz  # PyMuPDF\n",
        "from datetime import datetime\n",
        "\n",
        "def sanitize_filename(name):\n",
        "    return re.sub(r'[\\/:\"*?<>|]+', \"_\", name)\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        text = \"\"\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error extracting PDF {pdf_path.name}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def parse_xml_to_markdown_with_metadata(xml_path):\n",
        "    try:\n",
        "        with open(xml_path, 'rb') as f:\n",
        "            tree = etree.parse(f)\n",
        "\n",
        "        metadata = {\n",
        "            \"title\": \"\",\n",
        "            \"authors\": [],\n",
        "            \"doi\": \"\",\n",
        "        }\n",
        "\n",
        "        title_elem = tree.find(\".//article-title\")\n",
        "        if title_elem is not None:\n",
        "            full_title = title_elem.xpath(\"string()\").strip()\n",
        "            metadata[\"title\"] = full_title if full_title else xml_path.stem\n",
        "        else:\n",
        "            metadata[\"title\"] = xml_path.stem\n",
        "\n",
        "        doi_elem = tree.find(\".//article-id[@pub-id-type='doi']\")\n",
        "        if doi_elem is not None and doi_elem.text:\n",
        "            metadata[\"doi\"] = \"https://doi.org/\" + doi_elem.text.strip()\n",
        "\n",
        "        authors = []\n",
        "        for contrib in tree.findall(\".//contrib[@contrib-type='author']\"):\n",
        "            name = contrib.find('name')\n",
        "            if name is not None:\n",
        "                given = name.findtext('given-names', default='')\n",
        "                surname = name.findtext('surname', default='')\n",
        "                full_name = f\"{given} {surname}\".strip()\n",
        "                if full_name:\n",
        "                    authors.append(full_name)\n",
        "\n",
        "        metadata[\"authors\"] = \", \".join(authors)\n",
        "\n",
        "        sections = tree.xpath('//body//sec')\n",
        "        text_parts = []\n",
        "\n",
        "        for sec in sections:\n",
        "            title = sec.findtext('title')\n",
        "            if title:\n",
        "                text_parts.append(f\"### {title.strip()}\")\n",
        "            paragraphs = sec.findall('p')\n",
        "            for p in paragraphs:\n",
        "                if p.text and p.text.strip():\n",
        "                    text_parts.append(p.text.strip())\n",
        "\n",
        "        markdown_text = \"\\n\\n\".join(text_parts)\n",
        "        return markdown_text, metadata\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error parsing {xml_path.name}: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_input_path(input_path, output_dir):\n",
        "    input_path = pathlib.Path(input_path)\n",
        "    output_path = pathlib.Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    metadata_records = []\n",
        "\n",
        "    if input_path.is_file():\n",
        "        if input_path.suffix.lower() == \".xml\":\n",
        "            result = parse_xml_to_markdown_with_metadata(input_path)\n",
        "            if result:\n",
        "                raw_text, metadata = result\n",
        "                if raw_text.strip():\n",
        "                    final_name = sanitize_filename(input_path.stem) + \"_final.md\"\n",
        "                    final_path = output_path / final_name\n",
        "                    final_path.write_text(raw_text, encoding=\"utf-8\")\n",
        "                    metadata[\"filename\"] = final_path.name\n",
        "                    metadata_records.append((final_path, metadata))\n",
        "\n",
        "        elif input_path.suffix.lower() == \".pdf\":\n",
        "            text = extract_text_from_pdf(input_path)\n",
        "            if text:\n",
        "                # Extract better title from first few lines\n",
        "                first_lines = text.split('\\n')[:3]\n",
        "                title_candidate = next((line.strip() for line in first_lines if len(line.strip()) > 10), input_path.stem)\n",
        "                title_candidate = title_candidate.replace(\"_\", \" \").strip().title()\n",
        "\n",
        "                # Try to extract DOI using regex\n",
        "                doi_match = re.search(r\"(10\\.\\d{4,9}/[-._;()/:A-Z0-9]+)\", text, re.I)\n",
        "                doi = f\"https://doi.org/{doi_match.group(1)}\" if doi_match else \"\"\n",
        "\n",
        "                final_name = sanitize_filename(input_path.stem) + \"_final.md\"\n",
        "                final_path = output_path / final_name\n",
        "                final_path.write_text(text, encoding=\"utf-8\")\n",
        "\n",
        "                metadata = {\n",
        "                    \"title\": title_candidate,\n",
        "                    \"authors\": \"Unknown\",\n",
        "                    \"doi\": doi,\n",
        "                    \"filename\": final_path.name\n",
        "                }\n",
        "\n",
        "                metadata_records.append((final_path, metadata))\n",
        "\n",
        "    elif input_path.is_dir():\n",
        "        for file in input_path.glob(\"**/*\"):\n",
        "            metadata_records += process_input_path(file, output_dir)\n",
        "\n",
        "    return metadata_records"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 5:Create Vector Database**\n",
        "Process documents to store them as vectors, enabling question-answering with a retrieval system."
      ],
      "metadata": {
        "id": "9PLTgFRMgq8F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSSL2TQq3Tzy"
      },
      "outputs": [],
      "source": [
        "#  Load and Chunk Documents with Metadata\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_huggingface import HuggingFaceEmbeddings as HFEmbeddings\n",
        "\n",
        "\n",
        "def load_markdown_documents_with_metadata(metadata_records):\n",
        "    documents = []\n",
        "    for md_path, metadata in metadata_records:\n",
        "        text = md_path.read_text(encoding=\"utf-8\")\n",
        "        if text.strip():\n",
        "            doc = Document(page_content=text, metadata=metadata)\n",
        "            documents.append(doc)\n",
        "    return documents\n",
        "\n",
        "def hybrid_chunking(documents, threshold=3000):\n",
        "    chunks = []\n",
        "    for doc in documents:\n",
        "        if len(doc.page_content.strip()) <= threshold:\n",
        "            chunks.append(doc)\n",
        "        else:\n",
        "            splitter = RecursiveCharacterTextSplitter(chunk_size=1800, chunk_overlap=300)\n",
        "            split_docs = splitter.split_documents([doc])\n",
        "            for chunk in split_docs:\n",
        "                chunk.metadata.update(doc.metadata)\n",
        "            chunks.extend(split_docs)\n",
        "    return chunks\n",
        "\n",
        "def create_vector_database(chunks):\n",
        "    embeddings = HFEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
        "    vector_db = Chroma.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings,\n",
        "        collection_name=\"scientific_rag_db\",\n",
        "        persist_directory=\"/content/db\"\n",
        "    )\n",
        "    return vector_db\n",
        "\n",
        "def create_retrieval_chain(vector_db):\n",
        "    llm = ChatGroq(\n",
        "        model=\"llama3-70b-8192\",\n",
        "        temperature=0.2,\n",
        "        max_tokens=512,\n",
        "        api_key=os.environ.get(\"GROQ_API_KEY\")\n",
        "    )\n",
        "    prompt_template = PromptTemplate.from_template(\n",
        "        '''You are a helpful research paper assistant. Use the following context to answer scientific questions. Use your own knowledge only if relevant.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:'''\n",
        "    )\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=vector_db.as_retriever(search_kwargs={\"k\": 3}),\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": prompt_template}\n",
        "    )\n",
        "    return qa_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 6: Execute pipeline for Processing and Retrieval**\n",
        "Runs the entire pipeline from downloading scientific papers to processing them, creating a vector database, and setting up the question-answering system."
      ],
      "metadata": {
        "id": "goQWf9AGhXjY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4pwnlXv3WcT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d8ffa0-fff7-4073-d7dd-8649f6d540c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Enter path to a PDF/XML file or folder (e.g., /content/data_climate): /content/data_climate\n",
            " RAG System Ready.\n"
          ]
        }
      ],
      "source": [
        "user_path = input(\"📂 Enter path to a PDF/XML file or folder (e.g., /content/data_climate): \").strip()\n",
        "markdown_dir = \"/content/markdowns\"\n",
        "os.makedirs(markdown_dir, exist_ok=True)\n",
        "\n",
        "metadata_records = process_input_path(user_path, markdown_dir)\n",
        "docs = load_markdown_documents_with_metadata(metadata_records)\n",
        "chunks = hybrid_chunking(docs)\n",
        "vector_db = create_vector_database(chunks)\n",
        "qa_chain = create_retrieval_chain(vector_db)\n",
        "print(\" RAG System Ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 7: Query your newly created RAG/LLM**\n",
        "Allow users to ask scientific questions and get answers based on the documents stored in the vector database\n",
        "---\n",
        "### Examples of the questions to be ask\n",
        "\n",
        "* Why?What?How?\n",
        "* key-finding of the paper."
      ],
      "metadata": {
        "id": "cKn8Ptvjhtgj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRKhFuq93b2s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "06121178-7d33-49fc-e6ed-2cd8782ce99d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 Ask a scientific question (or type 'quit'): what is the effect of climate on nutritional status?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Answer:\n\nBased on the provided context, climate change events can have a negative impact on nutritional status, particularly among vulnerable populations such as children and adults. The effects of climate change on nutritional status can be influenced by personal and socio-demographic, economic, and environmental factors.\n\nWhile the context does not provide a direct answer to the question, it suggests that climate change can lead to poor nutritional status, which can further exacerbate health problems. The references provided also support this notion, highlighting the impact of climate change on child health and nutrition.\n\nFor example, Reference 3 (Helldén et al., 2021) mentions the importance of considering the effects of climate change on child health, including nutrition. Similarly, Reference 1 (Bhutta et al., 2019) emphasizes the need for paediatricians to address the impacts of climate change on child health, which likely includes nutritional status.\n\nIn summary, while the context does not provide a direct answer, it implies that climate change can have a negative impact on nutritional status, particularly among vulnerable populations.\n\n**Sources:**\n- [Effect of climate change on the health and nutritional status of children and their families in Africa: Scoping review](https://doi.org/10.1371/journal.pgph.0004897)\n- [Plos Global Public Health | Https://Doi.Org/10.1371/Journal.Pgph.0004897  July 14, 2025](https://doi.org/10.1371/journal.pgph.0004897)"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 Ask a scientific question (or type 'quit'): how does climate changes health of a person?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Answer:\n\nBased on the provided context, climate change affects human health in various ways, including:\n\n1. **Disasters**: Climate change is associated with disasters like droughts, floods, temperature changes, and changing vector patterns, which can lead to a range of health problems.\n2. **Child health**: Climate change can magnify existing vulnerabilities in children, leading to an estimated 88% of the disease burden. The effects of climate change on child health travel through many different pathways and vary significantly across geographical locations.\n3. **Pregnant women**: Extreme weather changes during pregnancy have been associated with an increased risk of preterm birth, partly attributable to water scarcity, which can have implications for the health of the neonate and the development of the child.\n4. **Malnutrition**: Climate change can exacerbate malnutrition, which is a leading factor in child morbidity and mortality.\n5. **Mortality**: Weather variability has been reported to increase the risk of overall mortality in children, particularly in infants.\n\nOverall, climate change can have far-reaching consequences for human health, particularly for vulnerable populations such as children and pregnant women.\n\n**Sources:**\n- [Plos Global Public Health | Https://Doi.Org/10.1371/Journal.Pgph.0004897  July 14, 2025](https://doi.org/10.1371/journal.pgph.0004897)\n- [Navigating parenthood in a climate change era: determinants of childbearing intentions in Iran](https://doi.org/10.1038/s41598-025-11708-1)"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 Ask a scientific question (or type 'quit'): effects of climate change in africa\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Answer:\n\nBased on the provided context, the effects of climate change in Africa include:\n\n1. Malnutrition\n2. Infectious diseases\n3. Respiratory diseases in children and adults\n4. Adverse pregnancy and birth outcomes\n5. High child and maternal morbidity and mortality\n6. Mental health problems\n\nThese health conditions are associated with various climatic change phenomena or events, such as:\n\n1. High temperatures\n2. Drought\n3. Floods\n4. Wildfires\n5. Air pollution\n\nAdditionally, the review highlights that personal and socio-demographic, economic, and environmental factors increase the risk of people to the effects of climate change events on poor nutritional and health status.\n\n**Sources:**\n- [Navigating Parenthood In A Climate](https://doi.org/10.1038/s41598-025-11708-1)\n- [Plos Global Public Health | Https://Doi.Org/10.1371/Journal.Pgph.0004897  July 14, 2025](https://doi.org/10.1371/journal.pgph.0004897)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Ask a scientific question (or type 'quit'): quit\n",
            " Q&A Markdown log saved to /content/QA_Log.md\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "qa_log_md = \"/content/QA_Log.md\"\n",
        "with open(qa_log_md, \"w\", encoding=\"utf-8\") as log_file:\n",
        "    log_file.write(f\"# Q&A Log - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"🧠 Ask a scientific question (or type 'quit'): \").strip()\n",
        "        if query.lower() == \"quit\":\n",
        "            print(f\" Q&A Markdown log saved to {qa_log_md}\")\n",
        "            break\n",
        "\n",
        "        result = qa_chain.invoke({\"query\": query})\n",
        "        answer = result.get(\"result\", \"\")\n",
        "\n",
        "        # Get top 2 sources\n",
        "        source_lines = []\n",
        "        top_sources = result.get(\"source_documents\", [])[:2]\n",
        "        for doc in top_sources:\n",
        "            title = doc.metadata.get(\"title\", \"\").strip()\n",
        "            if not title or title.lower() in [\"untitled\", \"fulltext\"]:\n",
        "                title = doc.metadata.get(\"filename\", \"\").replace(\"_final.md\", \"\").strip()\n",
        "\n",
        "            doi = doc.metadata.get(\"doi\", \"\").strip()\n",
        "            source_lines.append(f\"- [{title}]({doi})\" if doi else f\"- {title}\")\n",
        "\n",
        "        sources_md = \"\\n\".join(source_lines)\n",
        "\n",
        "        # Display Answer + Sources in Markdown format\n",
        "        display(Markdown(f\"### Answer:\\n\\n{answer}\\n\\n**Sources:**\\n{sources_md}\"))\n",
        "\n",
        "        # Save to Q&A Markdown log\n",
        "        with open(qa_log_md, \"a\", encoding=\"utf-8\") as log_file:\n",
        "            log_file.write(f\"### Question:\\n{query}\\n\\n\")\n",
        "            log_file.write(f\"### Answer:\\n{answer}\\n\\n\")\n",
        "            if sources_md:\n",
        "                log_file.write(\"**Sources:**\\n\" + sources_md + \"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **(OPTIONAL)Step 8:CONVERT Q&A LOG TO PDF**"
      ],
      "metadata": {
        "id": "5h1FhbatPqcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import markdown2\n",
        "from weasyprint import HTML\n",
        "from IPython.display import FileLink, display\n",
        "\n",
        "qa_log_pdf = \"/content/QA_Log.pdf\"\n",
        "html_text = markdown2.markdown_path(qa_log_md)\n",
        "HTML(string=html_text).write_pdf(qa_log_pdf)\n",
        "\n",
        "print(f\"✅ PDF Q&A log saved at: {qa_log_pdf}\")\n",
        "display(FileLink(qa_log_pdf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "i5iXgh2ZMVUp",
        "outputId": "ef9cef00-8806-467d-b1d8-7cbf7ce2feb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:fontTools.ttLib.ttFont:Reading 'maxp' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'maxp' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.004s to load 'maxp'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'maxp'\n",
            "INFO:fontTools.subset:maxp pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'cmap' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'cmap' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'post' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'post' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.007s to load 'cmap'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'cmap'\n",
            "INFO:fontTools.subset:cmap pruned\n",
            "INFO:fontTools.subset:fpgm dropped\n",
            "INFO:fontTools.subset:prep dropped\n",
            "INFO:fontTools.subset:cvt  dropped\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to load 'post'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'post'\n",
            "INFO:fontTools.subset:post pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'glyf' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'glyf' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'loca' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'loca' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'head' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'head' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.005s to load 'glyf'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'glyf'\n",
            "INFO:fontTools.subset:glyf pruned\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to close glyph list over 'cmap'\n",
            "INFO:fontTools.subset:Added gid0 to subset\n",
            "INFO:fontTools.subset:Closing glyph list over 'glyf': 38 glyphs before\n",
            "INFO:fontTools.subset:Glyph names: ['.notdef', 'A', 'C', 'D', 'L', 'M', 'P', 'Q', 'S', 'a', 'ampersand', 'c', 'colon', 'd', 'e', 'eight', 'five', 'four', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'one', 'r', 's', 'seven', 'six', 't', 'two', 'u', 'uni00A0', 'uni00AD', 'w', 'y', 'zero']\n",
            "INFO:fontTools.subset:Glyph IDs:   [0, 3, 9, 16, 19, 20, 21, 23, 24, 25, 26, 27, 29, 36, 38, 39, 47, 48, 51, 52, 54, 68, 70, 71, 72, 74, 75, 76, 79, 80, 81, 82, 85, 86, 87, 88, 90, 92]\n",
            "INFO:fontTools.subset:Closed glyph list over 'glyf': 38 glyphs after\n",
            "INFO:fontTools.subset:Glyph names: ['.notdef', 'A', 'C', 'D', 'L', 'M', 'P', 'Q', 'S', 'a', 'ampersand', 'c', 'colon', 'd', 'e', 'eight', 'five', 'four', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'one', 'r', 's', 'seven', 'six', 't', 'two', 'u', 'uni00A0', 'uni00AD', 'w', 'y', 'zero']\n",
            "INFO:fontTools.subset:Glyph IDs:   [0, 3, 9, 16, 19, 20, 21, 23, 24, 25, 26, 27, 29, 36, 38, 39, 47, 48, 51, 52, 54, 68, 70, 71, 72, 74, 75, 76, 79, 80, 81, 82, 85, 86, 87, 88, 90, 92]\n",
            "DEBUG:fontTools.subset.timer:Took 0.003s to close glyph list over 'glyf'\n",
            "INFO:fontTools.subset:Retaining 38 glyphs\n",
            "INFO:fontTools.subset:head subsetting not needed\n",
            "INFO:fontTools.subset:hhea subsetting not needed\n",
            "INFO:fontTools.subset:maxp subsetting not needed\n",
            "INFO:fontTools.subset:OS/2 subsetting not needed\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'hmtx' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'hmtx' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'hhea' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'hhea' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.002s to subset 'hmtx'\n",
            "INFO:fontTools.subset:hmtx subsetted\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset 'cmap'\n",
            "INFO:fontTools.subset:cmap subsetted\n",
            "INFO:fontTools.subset:loca subsetting not needed\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset 'post'\n",
            "INFO:fontTools.subset:post subsetted\n",
            "INFO:fontTools.subset:gasp subsetting not needed\n",
            "INFO:fontTools.subset:FFTM NOT subset; don't know how to subset\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'GDEF' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'GDEF' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.002s to subset 'GDEF'\n",
            "INFO:fontTools.subset:GDEF subsetted\n",
            "INFO:fontTools.subset:name subsetting not needed\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset 'glyf'\n",
            "INFO:fontTools.subset:glyf subsetted\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset GlyphOrder\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'head'\n",
            "INFO:fontTools.subset:head pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'OS/2' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'OS/2' table\n",
            "INFO:fontTools.subset:OS/2 Unicode ranges pruned: [0, 1]\n",
            "INFO:fontTools.subset:OS/2 CodePage ranges pruned: [0]\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'glyf'\n",
            "INFO:fontTools.subset:glyf pruned\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'GDEF'\n",
            "INFO:fontTools.subset:GDEF pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'name' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'name' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'gasp' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'gasp' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'FFTM' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'FFTM' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.004s to prune 'name'\n",
            "INFO:fontTools.subset:name pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'maxp' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'maxp' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.001s to load 'maxp'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'maxp'\n",
            "INFO:fontTools.subset:maxp pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'cmap' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'cmap' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'post' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'post' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.004s to load 'cmap'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'cmap'\n",
            "INFO:fontTools.subset:cmap pruned\n",
            "INFO:fontTools.subset:fpgm dropped\n",
            "INFO:fontTools.subset:prep dropped\n",
            "INFO:fontTools.subset:cvt  dropped\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to load 'post'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'post'\n",
            "INFO:fontTools.subset:post pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'glyf' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'glyf' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'loca' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'loca' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'head' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'head' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.006s to load 'glyf'\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'glyf'\n",
            "INFO:fontTools.subset:glyf pruned\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to close glyph list over 'cmap'\n",
            "INFO:fontTools.subset:Added gid0 to subset\n",
            "INFO:fontTools.subset:Closing glyph list over 'glyf': 66 glyphs before\n",
            "INFO:fontTools.subset:Glyph names: ['.notdef', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'W', 'a', 'b', 'bar', 'c', 'colon', 'comma', 'd', 'e', 'eacute', 'eight', 'f', 'five', 'four', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'nine', 'o', 'one', 'p', 'parenleft', 'parenright', 'percent', 'period', 'q', 'question', 'r', 's', 'seven', 'six', 'slash', 't', 'three', 'two', 'u', 'uni00A0', 'uni00AD', 'v', 'w', 'x', 'y', 'z', 'zero']\n",
            "INFO:fontTools.subset:Glyph IDs:   [0, 3, 8, 11, 12, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 48, 49, 50, 51, 53, 54, 55, 58, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 169]\n",
            "INFO:fontTools.subset:Closed glyph list over 'glyf': 67 glyphs after\n",
            "INFO:fontTools.subset:Glyph names: ['.notdef', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'W', 'a', 'acute', 'b', 'bar', 'c', 'colon', 'comma', 'd', 'e', 'eacute', 'eight', 'f', 'five', 'four', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'nine', 'o', 'one', 'p', 'parenleft', 'parenright', 'percent', 'period', 'q', 'question', 'r', 's', 'seven', 'six', 'slash', 't', 'three', 'two', 'u', 'uni00A0', 'uni00AD', 'v', 'w', 'x', 'y', 'z', 'zero']\n",
            "INFO:fontTools.subset:Glyph IDs:   [0, 3, 8, 11, 12, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 48, 49, 50, 51, 53, 54, 55, 58, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 116, 169]\n",
            "DEBUG:fontTools.subset.timer:Took 0.003s to close glyph list over 'glyf'\n",
            "INFO:fontTools.subset:Retaining 67 glyphs\n",
            "INFO:fontTools.subset:head subsetting not needed\n",
            "INFO:fontTools.subset:hhea subsetting not needed\n",
            "INFO:fontTools.subset:maxp subsetting not needed\n",
            "INFO:fontTools.subset:OS/2 subsetting not needed\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'hmtx' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'hmtx' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'hhea' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'hhea' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.002s to subset 'hmtx'\n",
            "INFO:fontTools.subset:hmtx subsetted\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset 'cmap'\n",
            "INFO:fontTools.subset:cmap subsetted\n",
            "INFO:fontTools.subset:loca subsetting not needed\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset 'post'\n",
            "INFO:fontTools.subset:post subsetted\n",
            "INFO:fontTools.subset:gasp subsetting not needed\n",
            "INFO:fontTools.subset:FFTM NOT subset; don't know how to subset\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'GDEF' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'GDEF' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.001s to subset 'GDEF'\n",
            "INFO:fontTools.subset:GDEF subsetted\n",
            "INFO:fontTools.subset:name subsetting not needed\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset 'glyf'\n",
            "INFO:fontTools.subset:glyf subsetted\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to subset GlyphOrder\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'head'\n",
            "INFO:fontTools.subset:head pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'OS/2' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'OS/2' table\n",
            "INFO:fontTools.subset:OS/2 Unicode ranges pruned: [0, 1]\n",
            "INFO:fontTools.subset:OS/2 CodePage ranges pruned: [0]\n",
            "DEBUG:fontTools.subset.timer:Took 0.001s to prune 'glyf'\n",
            "INFO:fontTools.subset:glyf pruned\n",
            "DEBUG:fontTools.subset.timer:Took 0.000s to prune 'GDEF'\n",
            "INFO:fontTools.subset:GDEF pruned\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'name' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'name' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'gasp' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'gasp' table\n",
            "DEBUG:fontTools.ttLib.ttFont:Reading 'FFTM' table from disk\n",
            "DEBUG:fontTools.ttLib.ttFont:Decompiling 'FFTM' table\n",
            "DEBUG:fontTools.subset.timer:Took 0.003s to prune 'name'\n",
            "INFO:fontTools.subset:name pruned\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ PDF Q&A log saved at: /content/QA_Log.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/content/QA_Log.pdf"
            ],
            "text/html": [
              "<a href='/content/QA_Log.pdf' target='_blank'>/content/QA_Log.pdf</a><br>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In this Colab notebook, we built a scientific RAG (Retrieval-Augmented Generation) pipeline that extracts information from PDF/XML-formatted research papers focused on biodiversity, wildlife, phytochemicals, and conservation. We parsed these XMLs into structured Markdown, embedded them using all-mpnet-base-v2, and connected them to a powerful LLM (LLaMA3-70B via Groq). The assistant can now accurately answer questions about scientific names, compounds, study locations, methodologies, and research findings — all grounded in real literature."
      ],
      "metadata": {
        "id": "6Fk58rY--Egj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**References:**\n",
        "- Garg A, Smith-Unna R D and Mu\n",
        "rray-Rust P, (pygetpapers:\n",
        "A   Python   library   for   automated   retrieval   of   scientific\n",
        "literature,Journal  of  Open  Source  Software,7(75)(2022)4451. https://doi.org/10.21105/joss.04451\n",
        "\n",
        "- [groqcloud](https://groq.com/groqcloud/)\n"
      ],
      "metadata": {
        "id": "L4smNvspANBv"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}